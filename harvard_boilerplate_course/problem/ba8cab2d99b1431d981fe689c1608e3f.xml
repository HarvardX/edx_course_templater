<problem display_name="Question Title" markdown="null">
  <!-- This problem only logs student responses. It grades nearly anything as correct. -->

  <script type="loncapa/python">
    
# The CDATA declaration lets us use less-than signs
# without edX thinking that we're starting an XML tag.

from python_lib import HXGraders

def answercheck(e, ans):

    options = {
        'min_length': 10,    # This is the minimum length in characters for each response.
        'fill_all': False   # Do all text boxes need an entry in order to save?
    }

    # This pulls from the python_lib.zip file in Files &amp; Uploads.
    return HXGraders.multiTextResponseGrader(ans, options)


</script>

<script type="text/javascript">
$(document).ready(function(){
	console.log('outer ready');
});

function logThatThing(ThatThing){
	console.log(JSON.stringify(ThatThing));
  // Call the edX logging code.
	Logger.log("harvardx.Boilerplate.multi_text", ThatThing);
}
</script>


<p>Put your prompt here. Use as many copies as you want of the &lt;textarea&gt; tag below to
  let learners type into boxes. You can change the height and width (cols and rows),
  put the textarea tag inside a table, switch it from "block" to "inline" to put it
  mid-sentence, etc.</p>

<textarea class="multianswer" name="answer" style="display:block;" rows="1" cols="70">Answer 1</textarea>
  <br/>
  <p>You can put multiple prompts if you want, or even make them small and put them into a table.</p>
  
<textarea class="multianswer" name="answer" style="display:block;" rows="2" cols="40">Answer 2</textarea>

<customresponse cfn="answercheck">
  <jsinput title="Multi-Text Response Exercise" gradefn="multi_text.getGrade" get_statefn="multi_text.getState" set_statefn="multi_text.setState" width="1" height="1" html_file="/static/multi_text.html" sop="false"/>
</customresponse>

<solution>
<div class="detailed-solution">
<p>Explanation</p>
<p>Questions should have explanations, or at least commentary.</p>
</div>
</solution>

</problem>
